---
title: "Lab Chapter 8"
output:
  html_document:
    df_print: paged
---
```{r,echo=FALSE}
set.seed(1)
library(tree)
library(ISLR)
library(MASS)
library(randomForest)
library(gbm)
library(ggplot2)
```

**7. In the lab, we applied random forests to the Boston data using mtry=6
and using ntree=25 and ntree=500. Create a plot displaying the test
error resulting from random forests on this data set for a more comprehensive
range of values for mtry and ntree. You can model your
plot after Figure 8.10. Describe the results obtained.**
```{r}
#Preparing data for use
attach(Boston)
#Creating training/test set
train <- sample(1:nrow(Boston), nrow(Boston)/2) #I don't like this training set size, but for consistency, will use same as in Lab.
boston_test <- Boston[-train,"medv"]
#Initializing matrix of variables sampled, tree count, and test MSE
Analysis <- matrix(
  c("","",""),
  nrow=1,
  ncol=3)
colnames(Analysis) <- c("nVariables","nTrees","MSE")
#Initializing matrix of test MSE for heatmap, rowname, and colname vectors
Z <- matrix(
  nrow=13,
  ncol=12
)
rownames(Z) <- c(200,250,300,350,400,450,500,550,600,650,700,750,800)
colnames(Z) <- c(2,3,4,5,6,7,8,9,10,11,12,13)
r_z <- c()
c_z <- c()
#Defining and recording how many variables sampled
for(v in 1:12){
  V <- v+1
  #Building colname vector
  c_z <- c(c_z,V)
  #Defining and recording how many trees grown
  for(n in 1:13){
    N <- 50*(n+3)
    #Building rowname vector
    if(v==1)r_z <- c(r_z,N)
    #Growing Random Forest
    rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry = V, ntree=N, importance=TRUE)
    #checking and recording MSE on test set
    rf_pred <- predict(rf.boston, newdata=Boston[-train,])
    E <- mean((rf_pred - boston_test)^2)
    #Inputting values wanted for analysis of random forests into Matrix
    Analysis <- rbind(Analysis,c(V,N,E))
    #Inputting value wanted for heatmap into Matrix
    Z[n,v] <- E
  }
}
#Removing first (empty) row of Matrix
Analysis <- Analysis[-1,]
```
```{r,echo=False}
print("Top 10 random forests based on MSE:")
```
```{r}
#Ordering Matrix based on error rate
Analysis <- Analysis[ order(Analysis[,3],decreasing=FALSE), ]
Analysis[1:10,]
```
```{r,echo=False}
#REDO, HEATMAP SEEMS WRONG SOMEHOW...
print("Heatmap of MSE based on number of trees and number of variables:")
```
```{r}
Analysis <- data.frame(Analysis)
attach(Analysis)
nTrees_temp <- as.numeric(levels(nTrees))[nTrees]
Analysis$nTrees <- nTrees_temp
nVariables_temp <- as.numeric(levels(nVariables))[nVariables]
Analysis$nVariables <- nVariables_temp
attach(Analysis)
ggplot(Analysis, aes(x=nTrees, y=nVariables )) +
  geom_tile(aes(fill = MSE), color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  xlab("Number of Trees") +
  ylab("Number of Variables Sampled") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 12),
        plot.title = element_text(size=16),
        axis.title=element_text(size=14,face="bold"),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(fill = "Mean Squared Error") +
  scale_x_discrete() +
  scale_y_discrete()
```
```{r,echo=FALSE}
print("By the looks of it, the more influential factor for a good prediction is the number of trees, the sweet spot is around 300-500.")
```

**8. In the lab, a classification tree was applied to the Carseats data set after
converting Sales into a qualitative response variable. Now we will
seek to predict Sales using regression trees and related approaches,
treating the response as a quantitative variable.**
```{r}
attach(Carseats)
```

(a) Split the data set into a training set and a test set.
```{r}
train <- sample(1:nrow(Carseats), ((4/5)*nrow(Carseats)) ) #takess a random sample of 4/5 of all observations
Carseats_train <- Carseats[train,]
Carseats_test <- Carseats[-train,]
Sales_test <- Carseats_test$Sales
```

(b) Fit a regression tree to the training set. Plot the tree, and interpret
the results. What test MSE do you obtain?
```{r}
#fitting tree
tree.carseats_train <- tree(Sales~., Carseats, subset=train)
#make predictions
tree.carseats_pred <- predict(tree.carseats_train, newdata=Carseats_test)
x <- mean((tree.carseats_pred-Sales_test)^2)
```
```{r,echo=FALSE}
print(paste("Test MSE is",x,"; Root MSE is",sqrt(x),", indicating this model leads to test predictions within around",sqrt(x),"of the true Sales value for the location."))
```

(c) Use cross-validation in order to determine the optimal level of
tree complexity. Does pruning the tree improve the test MSE?
```{r}
#see if pruning will be effective
cv.carseats_train <- cv.tree(tree.carseats_train)
plot(cv.carseats_train$size, cv.carseats_train$dev, type='b')
```
```{r,echo=FALSE}
print(paste("Cross-validation seems to indicate that the deviation is lowest,",min(cv.carseats_train$dev),", at a size of",cv.carseats_train$size[which.min(cv.carseats_train$dev)],"."))
```
```{r}
#obtaining tree pruned as per recommmendation
prune.carseats_train <- prune.tree(tree.carseats_train, best=6)
#obrtaining test MSE
prune.carseats_pred <- predict(prune.carseats_train, newdata=Carseats_test)
x1 <- mean((prune.carseats_pred-Sales_test)^2)
```
```{r,echo=FALSE}
print(paste("Pruning the tree does improve the test MSE, from",x,"to",x1,"; but the improvement isn't by much."))
```

(d) Use the bagging approach in order to analyze this data. What
test MSE do you obtain? Use the importance() function to determine
which variables are most important.
```{r}
#grow training bagged tree
bag.carseats_train <- randomForest(Sales~., data=Carseats, subset=train, mtry=ncol(Carseats)-1, importance=TRUE)
#check test MSE
bag.carseats_pred <- predict(bag.carseats_train, newdata=Carseats_test)
x2 <- mean((bag.carseats_pred - Sales_test)^2)
```
```{r,echo=FALSE}
print(paste("The test MSE obtained from bagging is",x2,". This is much better than the regular tree or the pruned tree"))
```
```{r}
#checking variable importance
varImpPlot(bag.carseats_train)
importance(bag.carseats_train)
```
```{r,echo=FALSE}
print(paste("Here, the first column is the mean decrease of accuracy when this variable is excluded."))
print(paste("The second column is the total decrease of node impurity that results from splits over that variable, over all trees."))
print(paste("The two most important variables, by far, are Price and ShelveLoc."))
```

(e) Use random forests to analyze this data. What test MSE do you
obtain? Use the importance() function to determine which variables
are most important. Describe the effect of m, the number of
variables considered at each split, on the error rate
obtained.
```{r}
#using the method used in question 1:
#Initializing matrix of variables sampled, tree count, and test MSE
Analysis <- matrix(
  c("","",""),
  nrow=1,
  ncol=3)
colnames(Analysis) <- c("nVariables","nTrees","MSE")
#Initializing matrix of test MSE for heatmap, rowname, and colname vectors
Z <- matrix(
  nrow=13,
  ncol=9
)
rownames(Z) <- c(200,250,300,350,400,450,500,550,600,650,700,750,800)
colnames(Z) <- c(2,3,4,5,6,7,8,9,10)
r_z <- c()
c_z <- c()
#Defining and recording how many variables sampled
for(v in 1:9){
  V <- v+1
  #Building colname vector
  c_z <- c(c_z,V)
  #Defining and recording how many trees grown
  for(n in 1:13){
    N <- 50*(n+3)
    #Building rowname vector
    if(v==1)r_z <- c(r_z,N)
    #Growing Random Forest
    rf.carseats_train <- randomForest(Sales~., data=Carseats, subset=train, mtry = V, ntree=N, importance=TRUE, na.action=na.omit) #there are no na's in Carseats_train and Sales... no reason na.action should be necessary?
    #checking and recording MSE on test set
    rf.carseats_pred <- predict(rf.carseats_train, newdata=Carseats_test)
    E <- mean((rf.carseats_pred - Sales_test)^2)
    #Inputting values wanted for analysis of random forests into Matrix
    Analysis <- rbind(Analysis,c(V,N,E))
    #Inputting value wanted for heatmap into Matrix
    Z[n,v] <- E
  }
}
#Removing first (empty) row of Matrix
Analysis <- Analysis[-1,]
#Ordering Matrix based on error rate
```
```{r,echo=False}
print("Top 10 random forests based on MSE:")
```
```{r}
Analysis <- Analysis[ order(Analysis[,3],decreasing=FALSE), ]
Analysis[1:10,]
```
```{r,echo=False}
print("Heatmap of MSE based on number of trees and number of variables:")
```
```{r}
#REDO, HEATMAP SEEMS WRONG SOMEHOW...
plot_ly(x=r_z,y=c_z,z=Z,colors = colorRamp(c("black", "yellow")),type='heatmap')
```
```{r,echo=FALSE}
print("By the looks of it, the more influential factor for a good prediction is the number of trees, the sweet spot is around ")
```

**9. This problem involves the OJ data set which is part of the ISLR
package.**
```{r}
attach(OJ)
```

(a) Create a training set containing a random sample of 800 observations,
and a test set containing the remaining observations.
```{r}
train <- sample(nrow(OJ), 800)
OJ_train <- OJ[train,]
OJ_test <- OJ[-train,]
Purchase_test <- OJ_test$Purchase
```

(b) Fit a tree to the training data, with Purchase as the response
and the other variables as predictors. Use the summary() function
to produce summary statistics about the tree, and describe the
results obtained. What is the training error rate? How many
terminal nodes does the tree have?
```{r}
#grow a tree
tree.OJ_train <- tree(Purchase~., OJ, subset=train)
#summary of tree
summary(tree.OJ_train)
```
```{r,echo=FALSE}
print(paste("The training error rate is",(summary(tree.OJ_train)$misclass[1]/summary(tree.OJ_train)$misclass[2]),", and the tree has",summary(tree.OJ_train)$size," terminal nodes."))
```

(c) Type in the name of the tree object in order to get a detailed
text output. Pick one of the terminal nodes, and interpret the
information displayed.
```{r}
print(tree.OJ_train)
```
```{r,echo=FALSE}
print("The left-most leaf, here titled node 4), classifies all observations with LoyalCH<.50395 and also LoyalCH<.051325 to be MM. 62 Observations fall under this split, with a cv error of 10.240. The probability of correct classification is .98387.")
```

(d) Create a plot of the tree, and interpret the results.
```{r}
plot(tree.OJ_train)
text(tree.OJ_train)
```
```{r,echo=FALSE}
print("It seems that the top two predictors used are LoyaLCH and PriceDiff. ")
```

(e) Predict the response on the test data, and produce a confusion
matrix comparing the test labels to the predicted test labels.
What is the test error rate?
```{r}
tree.OJ_pred <- predict(tree.OJ_train, newdata=OJ_test, type='class')
X <- table(tree.OJ_pred,Purchase_test)
X
```
```{r,echo=FALSE}
print(paste("The test error rate is",(X[1,2]+X[2,1])/nrow(OJ_test),"."))
```

(f) Apply the cv.tree() function to the training set in order to
determine the optimal tree size.
```{r}
cv.OJ_train <- cv.tree(tree.OJ_train, FUN=prune.misclass)
cv.OJ_train
```
```{r,echo=FALSE}
print(paste("The best trees seem to be of size 8 and 2, with cross-validation error of",min(cv.OJ_train$dev),"."))
```

(g) Produce a plot with tree size on the x-axis and cross-validated
classification error rate on the y-axis.
```{r}
plot(cv.OJ_train$size,cv.OJ_train$dev,type='b')
```

(h) Which tree size corresponds to the lowest cross-validated classification
error rate?
```{r,echo=FALSE}
print("See question (f).")
```

(i) Produce a pruned tree corresponding to the optimal tree size
obtained using cross-validation. If cross-validation does not lead
to selection of a pruned tree, then create a pruned tree with five
terminal nodes.
```{r}
prune.OJ_train_2 <- prune.misclass(tree.OJ_train,best=2)
prune.OJ_train_8 <- prune.misclass(tree.OJ_train,best=8)
```

(j) Compare the training error rates between the pruned and unpruned
trees. Which is higher?
```{r}
Y <- table(tree.OJ_train$y,OJ_train$Purchase)
Y
Y_2 <- table(prune.OJ_train_2$y,OJ_train$Purchase)
Y_2
Y_8 <- table(prune.OJ_train_8$y,OJ_train$Purchase)
Y_8
```
```{r,echo=FALSE}
print("I may have done something wrong... The training error is 0 for unpruned tree and both pruned trees.")
```

(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r}
prune.OJ_pred_2 <- predict(prune.OJ_train_2, newdata=OJ_test, type='class')
X2 <- table(prune.OJ_pred_2,Purchase_test)
X2
prune.OJ_pred_8 <- predict(prune.OJ_train_8, newdata=OJ_test, type='class')
X8 <- table(prune.OJ_pred_8,Purchase_test)
X8
```
```{r,echo=FALSE}
print(paste("For both pruned trees, using sizes 2 and 8, the test error rates are,",(X2[1,2]+X2[2,1])/nrow(OJ_test),", and",(X8[1,2]+X8[2,1])/nrow(OJ_test),", respectively, whereas the unpruned tree has error rate",(X[1,2]+X[2,1])/nrow(OJ_test),". Pruning is not very useful in this example."))
```

**10. We now use boosting to predict Salary in the Hitters data set.**
```{r}
attach(Hitters)
```

(a) Remove the observations for whom the salary information is
unknown, and then log-transform the salaries.
```{r}
Hitters <- na.omit(Hitters) #the only na values are in the Salary column anyway, this will do
Hitters$Salary[Hitters$Salary==0] <- 1 #making all the zeroes into ones, so that log can be taken
Hitters$Salary <- log(Hitters$Salary)
```

(b) Create a training set consisting of the first 200 observations, and
a test set consisting of the remaining observations.
```{r}
train=1:200
Hitters_train <- Hitters[train,]
Hitters_test <- Hitters[-train,]
```

(c) Perform boosting on the training set with 1,000 trees for a range
of values of the shrinkage parameter $\lambda$. Produce a plot with
different shrinkage values on the $x$-axis and the corresponding
training set MSE on the $y$-axis.
```{r}
#Initializing a Matrix of $\lambda$, test MSE, training MSE
Matrix <- matrix(
  nrow=1,
  ncol=3
)
colnames(Matrix) <- c("lamda","test_MSE","training_MSE")
#Defining and recording lambda
for(i in 1:7){
  ifelse(i%%2==1,{
    lambda <- (.001)*10^((i-1)/2)
  },
  {
    lambda <- (.005)*10^(i/2-1)
  })
  #grow boosted tree
  boost.Hitters_train <- gbm(Salary~., data=Hitters_train, distribution="gaussian", n.trees=1000, interaction.depth=1, shrinkage=lambda)
  #obtain training set MSE
  boost_train_pred <- predict(boost.Hitters_train, newdata=Hitters_train, n.trees=1000)
  MSE_train <- mean((boost_train_pred-Hitters_train$Salary)^2)
  #obtaining test set MSE
  boost_pred <- predict(boost.Hitters_train, newdata=Hitters_test, n.trees=1000)
  MSE_test <- mean((boost_pred-Hitters_test$Salary)^2)
  #inputting wanted information into Matrix for plotting
  Matrix <- rbind(Matrix,c(lambda,MSE_test,MSE_train))
}
Matrix <- Matrix[-1,] #Removing first row
Matrix <- Matrix[order(Matrix[,1]),] #Put matrix in order by lambda
Matrix
#Make plot
plot(Matrix[,1],Matrix[,3],type='b',main="Train MSE by Lambda")
```

(d) Produce a plot with different shrinkage values on the x-axis and
the corresponding test set MSE on the y-axis.
```{r}
#Make plot
plot(Matrix[,1],Matrix[,2],type='b',main="Test MSE by Lambda")
```

(e) Compare the test MSE of boosting to the test MSE that results
from applying two of the regression approaches seen in
Chapters 3 and 6.
```{r}
#Performing Multiple Linear and Multiple Linear with Interaction
#Polynomial Regression
mlm.OJ_train <- lm(Hitters_train$Salary~., Hitters_train)
mlm.OJ_train <- lm(Hitters_train$Salary~AtBat+Hits+Walks+Years+CWalks+PutOuts+Assists, Hitters_train)
#Testing MSE
mlm.OJ_pred <- predict(mlm.OJ_train, newdata=Hitters_test)
mlm.MSE <- mean((mlm.OJ_pred-Hitters_test$Salary)^2)
#Multiple Linear with Interaction
Mlm.OJ_train <- lm(Hitters_train$Salary~.*., Hitters_train)
co <- summary(Mlm.OJ_train)$coefficients #look at coefficient table
co <- co[order(co[,4]),] #look at top p-value predictors
Mlm.OJ_train <- lm(Hitters_train$Salary~HmRun:CWalks+AtBat:Hits+Years:CHmRun+AtBat+CHmRun:Division+CWalks:NewLeague+RBI:NewLeague+CAtBat:CWalks+CRuns:CWalks+RBI:NewLeague+CRBI:Division+CAtBat:PutOuts+CHits:CRuns+League:Division+Runs:Years+HmRun:Years, Hitters_train) #creating new model using good predictors
#Testing MSE
Mlm.OJ_pred <- predict(Mlm.OJ_train, newdata=Hitters_test)
Mlm.MSE <- mean((Mlm.OJ_pred-Hitters_test$Salary)^2)
```
```{r,echo=FALSE}
print(paste("The boosted tree had a best test MSE of",Matrix[5,2],", whereas the multiple linear regression had a test MSE of",mlm.MSE,", and the multiple linear regression with interaction had a test MSE of",Mlm.MSE,"."))
```

(f) Which variables appear to be the most important predictors in
the boosted model?
```{r}
summary(boost.Hitters_train)
```
```{r,echo=FALSE}
print("The most important variables in the boosted models are Cruns, followed by Walks, then AtBat, PutOuts, Assists, and so forth.")
```

(g) Now apply bagging to the training set. What is the test set MSE
for this approach?
```{r}
#making training bagged tree
bag.OJ_train <- randomForest(Hitters_train$Salary~., data=Hitters_train, mtry=(ncol(Hitters)-1))
bag.OJ_train
#checking test MSE
bag.OJ_pred <- predict(bag.OJ_train, Hitters_test)
bag.MSE <- mean((bag.OJ_pred-Hitters_test$Salary)^2)
```
```{r,echo=FALSE}
print(paste("The test set for the bagged tree is",bag.MSE,". This is just a bit better than the boosted tree, which had MSE",Matrix[5,2],"."))
```

**11. This question uses the Caravan data set.**
```{r}
attach(Caravan)
```

(a) Create a training set consisting of the first 1,000 observations,
and a test set consisting of the remaining observations.
```{r}
train=1:1000
Caravan_train <- Caravan[train,]
Caravan_test <- Caravan[-train,]
Purchase_test <- Caravan_test$Purchase
```

(b) Fit a boosting model to the training set with Purchase as the
response and the other variables as predictors. Use 1,000 trees,
and a shrinkage value of 0.01. Which predictors appear to be
the most important?
```{r}
#prepare response vector, as needed for gbm()
Purch <- ifelse(Purchase=="Yes",1,0)
Purch_train <- Purch[train]
Purch_test <- Purch[-train]
#grow boosted tree
boost.Caravan_train <- gbm(Purch_train~.-Purchase, data=Caravan_train, distribution="bernoulli", n.trees=1000, interaction.depth=1, shrinkage=.01)
summary(boost.Caravan_train)
```
```{r,echo=FALSE}
print("The top predictors are PPERSAUT, MKOOPKLA, MOPLHOOG, MBERMIDD, PBRAND, ABRAND, and so on.")
```

(c) Use the boosting model to predict the response on the test data.
Predict that a person will make a purchase if the estimated probability
of purchase is greater than 20 %. Form a confusion matrix.
What fraction of the people predicted to make a purchase
do in fact make one? How does this compare with the results
obtained from applying KNN or logistic regression to this data
set?
```{r}
boost.Caravan_prob <- predict(boost.Caravan_train, newdata=Caravan_test, n.trees=1000, type='response')
boost.Caravan_pred <- ifelse(boost.Caravan_prob>.2,"Yes","No")
L <- table(boost.Caravan_pred,Purchase_test)
L
```
```{r,echo=FALSE}
print(paste(L[2,2]/(L[2,2]+L[2,1])," of the people predicted to make a purchase made a purchase."))
```
```{r}
#creating logistic regression
logistic.Caravan_train <- glm(Purchase~., data=Caravan_train, family=binomial)
summary(logistic.Caravan_train)
#improving logistic regression
logistic.Caravan_train <- glm(Purchase~MGEMOMV+MGODRK+MGODOV+MRELOV+PPERSAUT, data=Caravan_train, family=binomial)
#obtaining test error rate
logistic.Caravan_probs <- predict(logistic.Caravan_train, newdata=Caravan_test, type='response')
contrasts(Purchase)
logistic.Caravan_pred <- ifelse(logistic.Caravan_probs>.5,"Yes","No")
L1 <- table(logistic.Caravan_pred,Purchase_test)
#preparing response factor
Pur_train <- factor(Caravan_train$Purchase)
#creating KNN
knn.Caravan_pred <- knn(Caravan_train[,-86], Caravan_test[,-86], Pur_train, k=1) #don't include Purchase variable
L2 <- table(knn.Caravan_pred,Purchase_test)
```
```{r}
L
(L[1,1]+L[2,2])/(L[1,1]+L[1,2]+L[2,1]+L[2,2])
L1
(L1[1])/(L1[1]+L1[2])
L2
(L2[1,1]+L2[2,2])/(L2[1,1]+L2[1,2]+L2[2,1]+L2[2,2])
```
```{r,echo=FALSE}
print("Interesting: Logistic regression predicted all would not purchase, and had the least error rate.")
```

12. Apply boosting, bagging, and random forests to a data set of your
choice. Be sure to fit the models on a training set and to evaluate their
performance on a test set. How accurate are the results compared
to simple methods like linear or logistic regression? Which of these
approaches yields the best performance?
