---
title: "Lab Chapter 4"
output: html_notebook
---
**10. This question should be answered using the Weekly data set, which
is part of the ISLR package. This data is similar in nature to the
Smarket data from this chapter's lab, except that it contains 1, 089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.**
```{r}
install.packages('ISLR')
require('ISLR')
dataset <- Weekly
attach(dataset)
?Weekly
```

(a) Produce some numerical and graphical summaries of the Weekly
data. Do there appear to be any patterns?
```{r,include=FALSE}
#Preparing the correlogram
  panel.cor <- function(x, y, digits = 2, cex.cor, ...)
  {
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    # correlation coefficient
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste("cor= ", txt, sep = "")
    text(0.5, 0.6, txt)
    
    # p-value calculation
    p <- cor.test(x, y)$p.value
    txt2 <- format(c(p, 0.123456789), digits = digits)[1]
    txt2 <- paste("p= ", txt2, sep = "")
    if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
    text(0.5, 0.4, txt2)
  }
```
```{r}
#Making a correlogram
pairs(dataset, upper.panel = panel.cor)
```
```{r,echo=FALSE}
print("Year vs Volume seems exponential. All the lags have some outlier. Today vs Direction seem related. For the most part, the p-value with Lag vs Lag are very low. Lag vs VOlume and Year vs Volume have low p-values. ")
```

(b) Use the full data set to perform a logistic regression with
Direction as the response and the five lag variables plus Volume
as predictors. Use the summary function to print the results. Do
any of the predictors appear to be statistically significant? If so,
which ones?
```{r}
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=dataset,family=binomial)
summary(glm.fit)
```
```{r,echo=FALSE}
print("The only statistically significant predictor of these is Lag2, though p-value .02 is still pretty high.")
```

(c) Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you
about the types of mistakes made by logistic regression.
```{r}
#Obtaining probabilities from model
glm.probs=predict(glm.fit,type='response')
#Seeing how the dummy variable functions
contrasts(Direction)
#Fitting the model probabilities to existing observations
glm.pred=rep("Down",nrow(dataset))
glm.pred[glm.probs>0.5]="Up"
#Confusion matrix
table(glm.pred,Direction)
```
```{r,echo=FALSE}
print("The model predict Up when the market is actually Down. Furthermore, it predicts Up about 10 time s more than Down. This does not nearly reflect the truth, This is a bad model.")
```

(d) Now fit the logistic regression model using a training data period
from 1990 to 2008, with Lag2 as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions
for the held out data (that is, the data from 2009 and 2010).
```{r}
#Creating Training/Test sets
train=(Year<2009)
testset=dataset[!train,]
#Fitting model on training set
glm.fitTrain=glm(Direction~Lag2,data=dataset,family=binomial,subset=train)
#Obtaining modeled probabilities to test set observations
glm.probsTest=predict(glm.fitTrain,testset,type='response')
#Fitting model probabilities to existing test observations
glm.predTest=rep("Down",nrow(testset))
glm.predTest[glm.probsTest>0.5]="Up"
#Confusion Matrix
table(glm.predTest,testset$Direction)
```
```{r,echo=FALSE}
correct_rate.glm <- mean(glm.predTest!=testset$Direction)
print(paste("The ratio of correct predictions is",correct_rate.glm,"."))
```
 
(e) Repeat (d) using LDA.
```{r}
require(MASS)
#Fitting model on training set
lda.fitTrain=lda(Direction~Lag2,data=dataset,subset=train)
#Obtaining modeled probabilities to test set
lda.probsTest=predict(lda.fitTrain,testset,type='response')
#Obtaining categorization from proabability
lda.predTest <- as.data.frame(lda.probsTest)[1]
#Confusion matrix
table(lda.predTest$class,testset$Direction)
```
```{r,echo=FALSE}
correct_rate.lda <- mean(lda.predTest$class!=testset$Direction)
print(paste("The ratio of correct predictions is",correct_rate.lda,"."))
```

(f) Repeat (d) using QDA.
```{r}
#Fitting model on training data
qda.fitTrain=qda(Direction~Lag2,data=dataset,subset=train)
#Obtaining modeled probabilities to test set
qda.probsTest=predict(qda.fitTrain,testset,type='response')
#Obtaining categorization from probability
qda.predTest=rep("Down",nrow(testset))
qda.predTest[glm.probsTest>0.5]="Up"
#Confusion matrix
table(qda.predTest,testset$Direction)
```
```{r,echo=FALSE}
correct_rate.qda <- mean(qda.predTest!=testset$Direction)
print(paste("The ratio of correct predictions is",correct_rate.qda,"."))
```

(g) Repeat (d) using KNN with K = 1.
```{r}
#Creating required inputs: trainset matrix, testset matrix, and training response variable
require(class)
trainset <- dataset[train,]
testset <- dataset[!train,]
train.X <- trainset$Lag2
train.X <- as.matrix(train.X)
test.X <- testset$Lag2
test.X <- as.matrix(test.X)
train.Direction <- trainset$Direction
#Performing K-Nearest Neighbors
set.seed(1)
knn.predTest <- knn(train.X,test.X,train.Direction,k=1)
#Confusion matrix
table(knn.predTest,testset$Direction)
```
```{r,echo=FALSE}
correct_rate.knn <- mean(knn.predTest!=testset$Direction)
print(paste("The ratio of correct predictions is",correct_rate.knn,"."))
```

(h) Which of these methods appears to provide the best results on
this data?
```{r,echo=FALSE}
print("In this case, the K-Nearest Neighbors method worked the best.")
```

(i) Experiment with different combinations of predictors, including
possible transformations and interactions, for each of the
methods. Report the variables, method, and associated confusion
matrix that appears to provide the best results on the held
out data. Note that you should also experiment with values for
K in the KNN classifier.
```{r}
#Removing non-numerical data entries to perform numerical transformations
dataset_num = dataset[,-9]
#Creating transformed datasets
log_dataset = log(dataset_num)
sqrt_dataset = sqrt(dataset_num)
sqr_dataset = as.data.frame(dataset_num^2)
cub_dataset = as.data.frame(dataset_num^3)
#Initializing matrix of type, method, interactions, K-value, and error rate:
Analysis <- matrix(
  c("Type","Method","Interactions","K-value","Error Rate"),
  nrow=1,
  ncol=5)
for(t in 1:5){
  x=c() #initializes x if it isn't, so it can be removed
  rm(x) #removes x to be used as next dataframe
  x=c() #re-initializes x
  if(t==1){
    x <- data.frame(dataset_num,Direction)
    type <- "normal"
  }
  else if(t==2){
    x <- data.frame(log_dataset,Direction)
    type <- "log"
  }
  else if(t==3){
    x <- data.frame(sqrt_dataset,Direction)
    type <- "square root"
  }
  else if(t==4){
    x <- data.frame(sqr_dataset,Direction)
    type <- "squared"
  }
  else{
    x <- data.frame(cub_dataset,Direction)
    type <- "cubed"
  }
  trainset=x[train,]
  testset=x[!train,]
  for(m in 1:7){
    if(m==1||m==2){
      method <- "logistic regression"
      #Fitting model on train set
      if(m %% 2 == 0){
        glm.fitTrain=glm( x$Direction~. , x , family=binomial , subset=train )
        interacion <- "No"
      }
      if(m %% 2 == 1){
        glm.fitTrain=glm( x$Direction~.*. , x , family=binomial , subset=train )
        interacion <- "Yes"
      }
      #Obtaining probabilities on test set
      glm.probsTest=predict( glm.fitTrain , testset , type='response' )
      #Predicting based on probabilities
      glm.predTest=rep("Down",nrow(testset))
      glm.predTest[glm.probsTest>0.5]="Up"
      #Error rate
      error_rate_glm <- mean(glm.predTest!=testset$Direction)
      err_rate <- error_rate_glm
      #Inputting basic values wanted for analysis of models into a table 
      Analysis <- c(Analysis,type,method,interaction,k-value,err_rate)
    }
    if(m==3||m==4){
      method <- "linear discriminant analysis"
      #Fitting model on train set
      if(m %% 2 == 0){
        lda.fitTrain=lda( x$Direction~. , x , subset=train )
        interacion <- "No"
      }
      if(m %% 2 == 1){
        lda.fitTrain=lda( x$Direction~.*. , x , subset=train )
        interacion <- "Yes"
      }
      #Obtaining probabilities on test set
      lda.probsTest=predict( lda.fitTrain , testset , type='response' )
      #Predicting based on probabilities
      lda.predTest <- as.data.frame(lda.probsTest)[1]
      #Error rate
      error_rate_lda <- mean(lda.predTest$class!=testset$Direction)
      err_rate <- error_rate_lda
      #Inputting basic values wanted for analysis of models into a table 
      Analysis <- c(Analysis,type,method,interaction,k-value,err_rate)
    }
    if(m==5||m==6){
      method <- "quadratic discriminant analysis"
      #Fitting model on train set
      if(m %% 2 == 0){
        qda.fitTrain=qda( x$Direction~. , x , subset=train )
        interacion <- "No"
      }
      if(m %% 2 == 1){
        qda.fitTrain=qda( x$Direction~.*. , x , subset=train )
        interacion <- "Yes"
      }
      #Obtaining probabilities on test set
      qda.probsTest=predict( qda.fitTrain , testset , type='response' )
      #Predicting based on probabilities
      qda.predTest <- as.data.frame(qda.probsTest)[1]
      #Error rate
      error_rate_qda <- mean(qda.predTest$class!=testset$Direction)
      err_rate <- error_rate_qda
      #Inputting basic values wanted for analysis of models into a table 
      Analysis <- c(Analysis,type,method,interaction,k-value,err_rate)
    }
    if(m==7){
      #Preparing different k-values
      for(K in 1:5){
        k-value <- K
        method = "k-nearest neighbors"
        interacion <- "N/A"
        #Preparing arguments for knn()
        train.X <- trainset[,-9]
        train.X <- as.matrix(train.X)
        test.X <- testset[,-9]
        test.X <- as.matrix(test.X)
        train.Direction <- trainset$Direction
        #Performing K-Nearest Neighbors Analysis
        set.seed(1)
        knn.predTest <- knn(train.X,test.X,train.Direction,k=K)
        table(knn.predTest,testset$Direction)
        error_rate_knn <- mean(qda.predTest$class!=testset$Direction)
        err_rate <- error_rate_knn
        #Inputting basic values wanted for analysis of models into a table 
        Analysis <- c(Analysis,type,method,interaction,k-value,err_rate)
      }
    }
  }
}
```
```{r,echo=FALSE}
print(Analysis)
```


**11. In this problem, you will develop a model to predict whether a given
car gets high or low gas mileage based on the Auto data set.**

(a) Create a binary variable, mpg01, that contains a 1 if mpg contains
a value above its median, and a 0 if mpg contains a value below
its median. You can compute the median using the median()
function. Note you may find it helpful to use the data.frame()
function to create a single data set containing both mpg01 and
the other Auto variables.

(b) Explore the data graphically in order to investigate the association
between mpg01 and the other features. Which of the other
features seem most likely to be useful in predicting mpg01? Scatterplots
and boxplots may be useful tools to answer this question.
Describe your findings.

(c) Split the data into a training set and a test set.

(d) Perform LDA on the training data in order to predict mpg01
using the variables that seemed most associated with mpg01 in
(b). What is the test error of the model obtained?

(e) Perform QDA on the training data in order to predict mpg01
using the variables that seemed most associated with mpg01 in
(b). What is the test error of the model obtained?

(f) Perform logistic regression on the training data in order to predict
mpg01 using the variables that seemed most associated with
mpg01 in (b). What is the test error of the model obtained?

(g) Perform KNN on the training data, with several values of K, in
order to predict mpg01. Use only the variables that seemed most
associated with mpg01 in (b). What test errors do you obtain?
Which value of K seems to perform the best on this data set?

**12. This problem involves writing functions.**

(a) Write a function, Power(), that prints out the result of raising 2
to the 3rd power. In other words, your function should compute
23 and print out the results.
Hint: Recall that x^a raises x to the power a. Use the print()
function to output the result.

(b) Create a new function, Power2(), that allows you to pass any
two numbers, x and a, and prints out the value of x^a. You can
do this by beginning your function with the line
> Power2 =function (x,a){
You should be able to call your function by entering, for instance,
> Power2 (3,8)
on the command line. This should output the value of 38, namely,
6, 561.

(c) Using the Power2() function that you just wrote, compute 103,
817, and 1313.

(d) Now create a new function, Power3(), that actually returns the
result x^a as an R object, rather than simply printing it to the
screen. That is, if you store the value x^a in an object called
result within your function, then you can simply return() this
return()
result, using the following line:
return (result )
The line above should be the last line in your function, before
the } symbol.

(e) Now using the Power3() function, create a plot of f(x) = x2.
The x-axis should display a range of integers from 1 to 10, and
the y-axis should display x2. Label the axes appropriately, and
use an appropriate title for the figure. Consider displaying either
the x-axis, the y-axis, or both on the log-scale. You can do this
by using log=''x'', log=''y'', or log=''xy'' as arguments to
the plot() function.

(f) Create a function, PlotPower(), that allows you to create a plot
of x against x^a for a fixed a and for a range of values of x. For
instance, if you call
> PlotPower (1:10 ,3)
then a plot should be created with an x-axis taking on values
1, 2, . . . , 10, and a y-axis taking on values 13, 23, . . . , 103.

13. Using the Boston data set, fit classification models in order to predict
whether a given suburb has a crime rate above or below the median.
Explore logistic regression, LDA, and KNN models using various subsets
of the predictors. Describe your findings.